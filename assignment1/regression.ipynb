{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models\n",
    "\n",
    "In this lab, we will explore various regression models for Boston Houses Dataset. We will use regression to predict Boston house prices. We will explore both Ordinary Least Squares and also explore other regression variant of popular classifiers such as decision trees and SVM.\n",
    "\n",
    "We will largely make use of the Scikit-Learn libraries (http://scikit-learn.org/stable/). You can find tutorials and user guide at http://scikit-learn.org/stable/documentation.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#Array processing\n",
    "import numpy as np\n",
    "\n",
    "#Data analysis, wrangling and common exploratory operations\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#For visualization. Matplotlib for basic viz and seaborn for more stylish figures + statistical figures not in MPL.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import Image\n",
    "\n",
    "from sklearn.datasets import load_boston                                                                       \n",
    "from sklearn.utils import shuffle                                                                                                                                                                      \n",
    "from sklearn import metrics                                                                                                  \n",
    "from sklearn import tree                                                                                                     \n",
    "from sklearn.tree import DecisionTreeRegressor                                                                                                             \n",
    "from sklearn.svm import SVC, LinearSVC , SVR                                                                                 \n",
    "from sklearn.linear_model import LinearRegression                                            \n",
    "from sklearn.ensemble import RandomForestRegressor                                                                                                          \n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV                                               \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pydot \n",
    "\n",
    "#######################End imports###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimension =  (506, 13)\n",
      "\n",
      "Attribute names =  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "\n",
      "The median values of house prices (in $1000's), max = 50.000, min = 5.000, average = 22.533,\n",
      "\n",
      " .. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Do not make any changes in this cell\n",
    "boston = load_boston()  \n",
    "print(\"Data dimension = \", boston.data.shape) \n",
    "print(\"\\nAttribute names = \", boston.feature_names)\n",
    "print(\"\\nThe median values of house prices (in $1000's), max = %.3f, min = %.3f, average = %.3f,\"\n",
    "      % (np.max(boston.target), np.min(boston.target), np.mean(boston.target)) ) \n",
    "print(\"\\n\", boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimension =  (506, 13)\n",
      "\n",
      "The first row of data =  [6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n",
      "\n",
      "Target dimension =  (506,)\n",
      "\n",
      "Before transformation:  711.0 0.0 70.07396704469443\n",
      "\n",
      "After transformation:  10.190454845432923 -4.6670204084548 2.4732713452985016e-15 2.9177492036731256 -1.931470986413033 3.5855223803197665e-16\n"
     ]
    }
   ],
   "source": [
    "#Do not make any changes in this cell.\n",
    "print(\"Data dimension = \", boston.data.shape)\n",
    "print(\"\\nThe first row of data = \", boston.data[0])\n",
    "print(\"\\nTarget dimension = \", boston.target.shape)\n",
    "print(\"\\nBefore transformation: \", np.max(boston.data), np.min(boston.data), np.mean(boston.data))\n",
    "\n",
    "#Split the data into the training set and the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=33)\n",
    "\n",
    "#Scale the data - important for regression. Learn what this function does\n",
    "scalerX = StandardScaler().fit(X_train)\n",
    "scalery = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "\n",
    "X_train = scalerX.transform(X_train)  \n",
    "y_train = scalery.transform(y_train.reshape(-1, 1))  \n",
    "X_test = scalerX.transform(X_test)    \n",
    "y_test = scalery.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(\"\\nAfter transformation: \", np.max(X_train), np.min(X_train), np.mean(X_train), np.max(y_train), np.min(y_train), np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task t1\n",
    "#Create 13 scatter plots such that variables (CRIM to LSTAT) are in X axis and MEDV in y-axis.\n",
    "#Organize the images such that the images are in 3 rows of 4 images each and 1 in last row\n",
    "\n",
    "#You can refer to http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "#to see how to create scatter plots\n",
    "\n",
    "#Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not make any change here\n",
    "#To make your life easy, I have created a function that \n",
    "# (a) takes a regressor object,(b) trains it (c) makes some prediction (d) evaluates the prediction\n",
    "def train_and_evaluate(clf, X_train, y_train): \n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Coefficient of determination on training set:\",clf.score(X_train, y_train))\n",
    "    cv = KFold(n_splits=5, random_state=1234, shuffle=True)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv)   \n",
    "    print(\"Average coefficient of determination using 5-fold crossvalidation:\",np.mean(scores))\n",
    "    \n",
    "def plot_regression_fit(actual, predicted):\n",
    "    plt.scatter(actual, predicted)\n",
    "    plt.plot([0, 50], [0, 50], '--k')\n",
    "    plt.axis('tight')\n",
    "    plt.xlabel('True price ($1000s)')\n",
    "    plt.ylabel('Predicted price ($1000s)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-409ca9abb46a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf_ols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;31m#change this line\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_ols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mclf_ols_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_ols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d890b46b5f50>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(clf, X_train, y_train)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# (a) takes a regressor object,(b) trains it (c) makes some prediction (d) evaluates the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Coefficient of determination on training set:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "#task t2\n",
    "#create a regressor object based on LinearRegression\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "#Change the following line as appropriate\n",
    "clf_ols = None #change this line\n",
    "\n",
    "train_and_evaluate(clf_ols,X_train,y_train) \n",
    "clf_ols_predicted = clf_ols.predict(X_test) \n",
    "\n",
    "#why using inverse_transform below?\n",
    "plot_regression_fit(scalery.inverse_transform(y_test), scalery.inverse_transform(clf_ols_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task t3\n",
    "#See http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html\n",
    "\n",
    "#Create a regression based on Support Vector Regressor. Set the kernel to linear\n",
    "#Change the following line as appropriate\n",
    "clf_svr= None   \n",
    "train_and_evaluate(clf_svr,X_train,y_train.ravel()) \n",
    "clf_svr_predicted = clf_svr.predict(X_test) \n",
    "plot_regression_fit(scalery.inverse_transform(y_test), scalery.inverse_transform(clf_svr_predicted))   \n",
    "\n",
    "#Create a regression based on Support Vector Regressor. Set the kernel to polynomial\n",
    "#Change the following line as appropriate\n",
    "clf_svr_poly= None\n",
    "train_and_evaluate(clf_svr_poly,X_train,y_train.ravel()) \n",
    "clf_svr_poly_predicted = clf_svr_poly.predict(X_test)      \n",
    "plot_regression_fit(scalery.inverse_transform(y_test), scalery.inverse_transform(clf_svr_poly_predicted)) \n",
    "\n",
    "#Create a regression based on Support Vector Regressor. Set the kernel to rbf\n",
    "#Change the following line as appropriate\n",
    "clf_svr_rbf= None \n",
    "train_and_evaluate(clf_svr_rbf,X_train,y_train.ravel())\n",
    "clf_svr_rbf_predicted = clf_svr_rbf.predict(X_test)    \n",
    "plot_regression_fit(scalery.inverse_transform(y_test), scalery.inverse_transform(clf_svr_rbf_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task t4\n",
    "#See http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html\n",
    "#Create regression tree\n",
    "#Change the following line as appropriate\n",
    "clf_cart = None  \n",
    "train_and_evaluate(clf_cart,X_train,y_train) \n",
    "clf_cart_predicted = clf_cart.predict(X_test)  \n",
    "plot_regression_fit(scalery.inverse_transform(y_test), scalery.inverse_transform(clf_cart_predicted))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task t5\n",
    "#See http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "#Create a random forest regressor with 10 estimators and random state as 1234\n",
    "#Change the following line as appropriate\n",
    "clf_rf= None\n",
    "train_and_evaluate(clf_rf,X_train,y_train.ravel())  \n",
    "\n",
    "#The following prints the most important features\n",
    "std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],axis=0)\n",
    "indices = np.argsort(clf_rf.feature_importances_)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"\\nFeature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d-%s (%f)\" % (f + 1, indices[f], boston.feature_names[indices[f]], clf_rf.feature_importances_[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), clf_rf.feature_importances_[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "clf_rf_predicted = clf_rf.predict(X_test)      \n",
    "plot_regression_fit(scalery.inverse_transform(y_test), scalery.inverse_transform(clf_rf_predicted)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
